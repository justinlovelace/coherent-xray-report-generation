{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as pl\n",
    "from report_parser import parse_report\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import gzip\n",
    "import random\n",
    "import spacy\n",
    "import gensim, logging\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "\n",
    "from report_parser import parse_report\n",
    "from google.cloud import storage\n",
    "\n",
    "PAD_CHAR = '**PAD**'\n",
    "UNK_CHAR = '**UNK**'\n",
    "START_CHAR = '**START**'\n",
    "END_CHAR = '**END**'\n",
    "\n",
    "dataset_file_path = ''\n",
    "local_file_path = ''\n",
    "chexpert_csv_file_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, reports):\n",
    "        self.reports = reports\n",
    "\n",
    "    def __iter__(self):\n",
    "        for report in self.reports:\n",
    "            #iterates over all tokens in each sentence in the report\n",
    "            yield [token.text for token in report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_studies = pd.read_csv(os.path.join(dataset_file_path, 'cxr-study-list.csv.gz'))\n",
    "df_studies.rename(columns={'path': 'report_path'}, inplace=True)\n",
    "print(df_studies.columns.tolist())\n",
    "\n",
    "print('Number of reports: ')\n",
    "print(len(df_studies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "processed_notes = {}\n",
    "report_indices = []\n",
    "for index, row in tqdm(df_studies.iterrows(), total=df_studies.shape[0]):\n",
    "    report_path = row['report_path']\n",
    "    parsed_rep = parse_report(os.path.join(dataset_file_path, report_path))\n",
    "    if 'findings' in parsed_rep:\n",
    "        if not parsed_rep['findings'] or parsed_rep['findings'].isspace():\n",
    "            print('Null section')\n",
    "            print(parsed_rep['findings'])\n",
    "        else:\n",
    "            tokenized = nlp(parsed_rep['findings'], disable=['ner'])\n",
    "            processed_notes[report_path]=tokenized\n",
    "            report_indices.append(index)\n",
    "#     print('\\n\\nFINDINGS...')\n",
    "#     print(parsed_rep['findings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of processed reports: ')\n",
    "print(len(processed_notes))\n",
    "\n",
    "print('DROPPING REPORTS WITHOUT FINDINGS')\n",
    "df_studies = df_studies.iloc[report_indices]\n",
    "df_studies.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('Number of reports with findings: ')\n",
    "print(len(df_studies))\n",
    "print(df_studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Splitting data')\n",
    "df_subjects = df_studies[['subject_id']].drop_duplicates()\n",
    "train_subjects, test_subjects = train_test_split(df_subjects, test_size=0.2, random_state=0)\n",
    "train_subjects, val_subjects = train_test_split(train_subjects, test_size=0.125, random_state=0)\n",
    "\n",
    "print('Total\\nNumber of subjects: {}\\tPercentage: {}'.format(len(df_subjects), len(df_subjects)/len(df_subjects)))\n",
    "print('Train\\nNumber of subjects: {}\\tPercentage: {}'.format(len(train_subjects), len(train_subjects)/len(df_subjects)))\n",
    "print('Val\\nNumber of subjects: {}\\tPercentage: {}'.format(len(val_subjects), len(val_subjects)/len(df_subjects)))\n",
    "print('Test\\nNumber of subjects: {}\\tPercentage: {}'.format(len(test_subjects), len(test_subjects)/len(df_subjects)))\n",
    "\n",
    "df_train = df_studies[df_studies.subject_id.isin(train_subjects.subject_id)]\n",
    "df_val = df_studies[df_studies.subject_id.isin(val_subjects.subject_id)]\n",
    "df_test = df_studies[df_studies.subject_id.isin(test_subjects.subject_id)]\n",
    "\n",
    "print('Total\\nNumber of studies: {}\\tPercentage: {}'.format(len(df_studies), len(df_studies)/len(df_studies)))\n",
    "print('Train\\nNumber of studies: {}\\tPercentage: {}'.format(len(df_train), len(df_train)/len(df_studies)))\n",
    "print('Val\\nNumber of studies: {}\\tPercentage: {}'.format(len(df_val), len(df_val)/len(df_studies)))\n",
    "print('Test\\nNumber of studies: {}\\tPercentage: {}'.format(len(df_test), len(df_test)/len(df_studies)))\n",
    "\n",
    "# print(df_train.head(1))\n",
    "# print(df_subjects.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_notes = []\n",
    "for index, row in tqdm(df_train.iterrows(), total=df_train.shape[0]):\n",
    "    train_notes.append(processed_notes[row['report_path']])\n",
    "\n",
    "\n",
    "report_iter = MySentences(train_notes)\n",
    "i=0\n",
    "for test in report_iter:\n",
    "    print('REPORT')\n",
    "    print(report_iter.reports[i])\n",
    "    print('REPORT TOKENS')\n",
    "    print(test)\n",
    "    print(type(test[0]))\n",
    "    i+=1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = gensim.models.Word2Vec(report_iter, min_count=5, workers=8, size=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "datasetPath = os.path.join(local_file_path, 'cxr_vectors.kv')\n",
    "word_vectors.save(datasetPath)\n",
    "\n",
    "datasetPath = os.path.join(local_file_path, 'cxr_w2v_model')\n",
    "model.save(datasetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "pad_emb = np.zeros((1, 256), dtype=np.float32)\n",
    "unk_emb = np.random.randn(1, 256)\n",
    "start_emb = np.random.randn(1, 256)\n",
    "end_emb = np.random.randn(1, 256)\n",
    "\n",
    "weights = model.wv.vectors\n",
    "unk_ind = weights.shape[0]\n",
    "print(unk_ind)\n",
    "print(weights.shape)\n",
    "print(weights.dtype)\n",
    "weights = np.concatenate((weights, unk_emb.astype(np.float32), end_emb.astype(np.float32), start_emb.astype(np.float32), pad_emb), axis=0)\n",
    "\n",
    "print(weights.shape)\n",
    "print(weights.dtype)\n",
    "\n",
    "datasetPath = os.path.join(local_file_path, 'cxr_w2v.npy')\n",
    "np.save(datasetPath, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, report in processed_notes.items():\n",
    "    for sentence in report.sents:\n",
    "        print(sentence)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind = {}\n",
    "ind2word = {}\n",
    "for idx, key in enumerate(word_vectors.vocab):\n",
    "            word2ind[key] = idx\n",
    "            ind2word[idx] = key\n",
    "word2ind[UNK_CHAR] = idx+1\n",
    "word2ind[END_CHAR] = idx+2\n",
    "word2ind[START_CHAR] = idx+3\n",
    "word2ind[PAD_CHAR] = idx+4\n",
    "ind2word[idx+1] = UNK_CHAR\n",
    "ind2word[idx+2] = END_CHAR\n",
    "ind2word[idx+3] = START_CHAR\n",
    "ind2word[idx+4] = PAD_CHAR\n",
    "\n",
    "datasetPath = os.path.join(local_file_path, 'word2ind.npy')\n",
    "np.save(datasetPath, word2ind)\n",
    "\n",
    "datasetPath = os.path.join(local_file_path, 'ind2word.npy')\n",
    "np.save(datasetPath, ind2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx=0\n",
    "csv_reports = []\n",
    "paths = []\n",
    "for report_path, report in tqdm(processed_notes.items()):\n",
    "    if idx < 3:\n",
    "        print('Report ' + str(idx))\n",
    "        print(report_path)\n",
    "    file_path = os.path.join(local_file_path, report_path)\n",
    "    Path(os.path.dirname(file_path)).mkdir(parents=True, exist_ok=True)\n",
    "    with open(file_path, 'w') as file:   \n",
    "        full_report = []\n",
    "        for sentence in report.sents:\n",
    "            sent = []\n",
    "            if idx < 3:\n",
    "                print(sentence)\n",
    "            for word in sentence:\n",
    "                if word.text in word_vectors.vocab:\n",
    "                    sent.append(word.text)\n",
    "                    full_report.append(word.text)\n",
    "                else:\n",
    "                    sent.append(UNK_CHAR)\n",
    "                    full_report.append(UNK_CHAR)\n",
    "            file.write(' '.join(x for x in sent))\n",
    "            file.write('\\n')\n",
    "            if idx < 3:\n",
    "                print(sent) \n",
    "        csv_reports.append([' '.join(x for x in full_report)])\n",
    "        paths.append([report_path])\n",
    "        if idx < 3:\n",
    "            print(full_report)\n",
    "            print(csv_reports[-1][0])\n",
    "        idx+=1\n",
    "        \n",
    "assert len(csv_reports) == len(paths)\n",
    "\n",
    "print('SAVING REPORTS TO CSV...')\n",
    "csv_file_path = os.path.join(chexpert_csv_file_path, 'reports.csv')\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    wr = csv.writer(csv_file, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(csv_reports)\n",
    "\n",
    "print('Saving filepath dataframe...')\n",
    "csv_file_path = os.path.join(chexpert_csv_file_path, 'report_paths.csv')\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    wr = csv.writer(csv_file, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerows(paths)\n",
    "#         if idx > 3:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_records = pd.read_csv(os.path.join(dataset_file_path, 'cxr-record-list.csv.gz'))\n",
    "df_records.rename(columns={'path': 'dicom_path'}, inplace=True)\n",
    "print(df_records.columns.tolist())\n",
    "\n",
    "print('Number of images: ')\n",
    "print(len(df_records))\n",
    "\n",
    "df_master_train = df_records.merge(df_train, how='inner', on=['subject_id', 'study_id'])\n",
    "\n",
    "print('Number of unique training reports: ')\n",
    "print(len(df_train))\n",
    "print('Number of training reports after matching to images: ')\n",
    "print(len(df_master_train))\n",
    "\n",
    "df_master_val = df_records.merge(df_val, how='inner', on=['subject_id', 'study_id'])\n",
    "\n",
    "print('Number of unique val reports: ')\n",
    "print(len(df_val))\n",
    "print('Number of val reports after matching to images: ')\n",
    "print(len(df_master_val))\n",
    "\n",
    "df_master_test = df_records.merge(df_test, how='inner', on=['subject_id', 'study_id'])\n",
    "\n",
    "print('Number of unique test reports: ')\n",
    "print(len(df_test))\n",
    "print('Number of test reports after matching to images: ')\n",
    "print(len(df_master_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading train df...')\n",
    "old_df_train = pd.read_csv(os.path.join(local_file_path, 'df_master_train.csv'))\n",
    "print(len(old_df_train))\n",
    "print('Loading val df...')\n",
    "old_df_val = pd.read_csv(os.path.join(local_file_path, 'df_master_val.csv'))\n",
    "print(len(old_df_val))\n",
    "print('Loading test df...')\n",
    "old_df_test = pd.read_csv(os.path.join(local_file_path, 'df_master_test.csv'))\n",
    "print(len(old_df_test))\n",
    "\n",
    "print('Concatenating dataframes...')\n",
    "df_master = pd.concat([old_df_train, old_df_val, old_df_test])\n",
    "\n",
    "print(df_master)\n",
    "print(len(df_master))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Merging train df...')\n",
    "df_master_train = df_master.merge(df_master_train[['study_id', 'dicom_id']], how = 'inner', on=['study_id', 'dicom_id'])\n",
    "print(len(df_master_train))\n",
    "# print(df_master_train)\n",
    "\n",
    "df_master_val = df_master.merge(df_master_val[['study_id', 'dicom_id']], how = 'inner', on=['study_id', 'dicom_id'])\n",
    "print(len(df_master_val))\n",
    "\n",
    "df_master_test = df_master.merge(df_master_test[['study_id', 'dicom_id']], how = 'inner', on=['study_id', 'dicom_id'])\n",
    "print(len(df_master_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving train dataframe...')\n",
    "datasetPath = os.path.join(local_file_path, 'df_master_train.csv')\n",
    "df_master_train.to_csv(datasetPath, index=False)\n",
    "\n",
    "print('Saving val dataframe...')\n",
    "datasetPath = os.path.join(local_file_path, 'df_master_val.csv')\n",
    "df_master_val.to_csv(datasetPath, index=False)\n",
    "\n",
    "print('Saving test dataframe...')\n",
    "datasetPath = os.path.join(local_file_path, 'df_master_test.csv')\n",
    "df_master_test.to_csv(datasetPath, index=False)\n",
    "\n",
    "print('Finished saving dataframes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}